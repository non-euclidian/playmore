{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaybk0LkQ6v5"
      },
      "source": [
        "# What's this?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQiFODXpOZSV"
      },
      "source": [
        "This is part I of **Playmore: Executable Transcript of Andrej Karpathy’s _“[Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)”_ Series**. Complete with timecodes.\n",
        "\n",
        "Part I covers building **`micrograd`**, a minimal neural net training library. Along the way you build an intuition of how neural networks learn and work. It shows _how_ the code in [Andrej's lecture](https://www.youtube.com/watch?v=VMj-3S1tku0) evolves, one idea at a time, following the linear order of the video. There's deliberately almost no code encapsulation and [DRY principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) is blatantly violated. This trade-off escapes the need to scroll up and down: all the relevant code is always in your sight.\n",
        "\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">But why?</h3></summary>\n",
        "  \n",
        "I started making these transcripts as a way to understand and review the material later. Then I realized this could be useful to others:\n",
        "   \n",
        "1. There's value in being able to follow the development of an idea through intuitions, trials and errors. It's a way of remembering the _why_ of how something works. Neat, polished, refactored final code doesn't do that. This notebook guides you through Andrej's intuitions that gradually lead to code without having to rewatch the video.\n",
        "2. I find that reading commentary and executing code is way more efficient to revisit ideas after a while than going through the video again.\n",
        "3. Some people like text more than video. Or want to supplement one with the other for a multimodal learning experience.\n",
        "\n",
        "\n",
        "If you want the final code, then Andrej published it in [this Github repo](https://github.com/karpathy/micrograd).\n",
        "\n",
        "</details>\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">Does this map 1:1 to the video?</h3></summary>\n",
        "\n",
        "\n",
        "Not word-by-word. Occasionally there's some extra code or metaphor I added to illustrate a point. Some parts are (*manually*) summarized and you should refer to the video for details. 99% of the content comes from Andrej. Any mistakes or inaccuracies are all mine (_please let me know!_). \n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">How to take the most out of this?</h3></summary>\n",
        "\n",
        ">\n",
        "> **_What I cannot create, I do not understand._**\n",
        ">\n",
        "> [Richard Feynman](https://journals.biologists.com/jcs/article/130/18/2941/56386/What-I-cannot-create-I-do-not-understand)\n",
        ">\n",
        "\n",
        "This is a _play-as-you-watch_ lecture supplement helping you grasp material more fully and revise it after a while. If the material is new, start by launching [the lecture](https://www.youtube.com/watch?v=VMj-3S1tku0). Use this notebook to test lecture ideas as they get developed.\n",
        "\n",
        "For maximum benefit, try to recreate the code for key ideas and write lecture notes of your own, aiming to express the underlying ideas precisely and poking holes in your reasoning.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">Why <i>Playmore</i>?</h3></summary>\n",
        "\n",
        "It's a reference to the _play_ button used to execute a `Jupyter` notebook cell. And a subtle invitation to learn by doing and playing, which is the most effective approach to master a subject. You don't learn to play tennis just by watching someone else do it - _gotta do it yourself!_\n",
        "\n",
        "</details>\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">Who are you?</h3></summary>\n",
        "\n",
        "I'm Andrey from [noneuclidianspace.com](https://noneuclidianspace.com), a programmer and fellow learner. Some 15 years ago I stumbled upon a yellow book titled _\"Artificial Neural Networks\"_ in a library by chance. I've been fascinated with AI ever since. I worked as a software architect, consultant, founder; built data aggregation systems and software that helps you become a better writer. Nowadays I'm thinking about how [Vannevar Bush's Memex](https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/) can be reimagined in the age of Transformer models. Reach out with ideas and feedback to **`playmore [at] noneuclidianspace.com`**.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details style=\"margin-bottom:0.75rem;\">\n",
        "<summary><h3 style=\"display: inline;\">License</h3></summary>\n",
        "\n",
        "MIT License\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsWdPGST3xKD"
      },
      "source": [
        "# Intro [00:00:00](https://www.youtube.com/watch?v=VMj-3S1tku0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTz6IGy32nX"
      },
      "source": [
        "In this tutorial you build a neural network from scratch, step-by-step. In the process you build an understanding of how it works and learns on an intuitive level. You end up with **[micrograd](https://github.com/karpathy/micrograd)** - a fully functional neural network training framework. It uses _backpropagation_, an algorithm applying derivatives to gently nudge neural network parameters towards values that minimize the prediction error. Backpropagation is a key part of any modern deep learning library, such as `PyTorch`, `Jax` and `Tensorflow`.\n",
        "\n",
        "At its core, building a neural network is extremely simple. It's about 250 lines of code. All the complexity in modern neural net frameworks comes from efficiency and optimizing for time and space.\n",
        "\n",
        "To follow this tutorial you only need basic knowledge of `Python` and vague recollection of calculus from high school."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm5LvgMxdRAH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s94YkU07bmMw"
      },
      "source": [
        "# So what's a derivative? Let's build an intuition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTpY1Ij8Jltp"
      },
      "source": [
        "Derivative tells us how some function $f(x)$ changes if we plug in different numbers $x$. Does it go up? Does it go down? How fast? Derivative has the answer. Essentially, it is the slope of that function at $x$.\n",
        "\n",
        "> _**History note**_: Using derivative as a tool for measuring rate of change was developed independently by famous mathematicians Isaac Newton in 1665-1666 and Gottfried Leibniz. They did it roughly around the same time. Newton came up with this idea first, but didn't rush to make it public. It was Leibnitz who published first, having developed the idea independently. This led to a [lifelong rivalry](https://pages.cs.wisc.edu/~sastry/hs323/calculus.pdf) over attribution and one of the biggest scandals in science history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJX3_U0Yh7sZ"
      },
      "source": [
        "### Derivative of a simple function with one input [00:08:08](https://www.youtube.com/watch?v=VMj-3S1tku0&t=488s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKsAP3_NgAlL"
      },
      "source": [
        "Let's define a simple function with one variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYkovAOTbf4m"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  return 3*x**2 - 4*x + 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_asRSuqdixH",
        "outputId": "a1ac77a8-1948-44ca-99a4-4b18726c43ae"
      },
      "outputs": [],
      "source": [
        "f(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZR3g_eK3cRX"
      },
      "source": [
        "This is just a randomly chosen function of one variable. In principle, you could pick another one, as long as it's _smooth_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BibBLksQgF-E"
      },
      "source": [
        "Now let's plot our $f(x)$ in the range $[-5;5]$ to get a feel of its shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt_5bf_B86T1",
        "outputId": "ea9aafbd-effe-4c69-8655-181bbac83e61"
      },
      "outputs": [],
      "source": [
        "xs = np.arange(-5, 5, 0.25) # a linear space made up of points between -5 and 5 with a step of 0.25\n",
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9-lEdfEHdjyq",
        "outputId": "909ead7c-f99c-4823-e96e-65e8c41c3abf"
      },
      "outputs": [],
      "source": [
        "ys = f(xs)\n",
        "plt.plot(xs, ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87QDWUO54Y1S"
      },
      "source": [
        "What is the derivative of any single point $x$ of this function? What does that even mean?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "If you took a calculus class, you probably took a piece of paper, applied all the rules and got a mathematical expression of the derivative.\n",
        "\n",
        "We are not going to do that. Noone in neural nets actually writes out an expression for the neural net. It's impractical - those nets are just _too_ big. Instead, we will focus on the intuition behind the derivative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kW9AkvM2LcN"
      },
      "source": [
        "We already pointed out that the derivative gives you function slope at a point. But how do you actually measure slope at point $x$?\n",
        "\n",
        "By introducing another point, $x+h$ spaced _very_ close to $x$. So close, that it's _almost_ the same point, _but not exactly_. As you move the second point closer to the first, the slope of the line connecting them (_secant_) becomes the slope of the _tangent line_ at\n",
        "$x$. This means $h$ should be a very small value - _infinitely small_, ideally. However, for most practical purposes, _pretty small_ works just fine.\n",
        "\n",
        "Armed with this idea, you find the slope at $x$ by computing the ratio $\\large\\frac{f(x+h)-f(x)}{h}$. Essentially, this gives you $\\large\\frac{rise}{run}$ over an incredibly small distance - so tiny that it's _nearly_ the same point.\n",
        "\n",
        "Let's do it in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D3PpaQDgrBS",
        "outputId": "2bcef425-5924-4360-f0ba-f7e46bc4a899"
      },
      "outputs": [],
      "source": [
        "h = 0.00001 # small enough for us\n",
        "x = 3.0\n",
        "print(f\"f(x+h)={f(x + h)}\")\n",
        "print(f\"Here's how much change we caused by increasing x just a tiny bit -> \" \\\n",
        "+f\"{f(x + h) - f(x)}\")\n",
        "print(f\"Here's the derivative value at x' = x+h -> \" \\\n",
        "+f\"{(f(x + h) - f(x))/h}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plvP9eBnPKqt"
      },
      "source": [
        "As $h$ becomes closer to zero, our estimate of function slope at $x$ becomes more precise. There you go: these are the key intuitions that lead to [formal definition of the derivative using limits](https://en.wikipedia.org/wiki/Derivative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiH7f1K46NrI"
      },
      "source": [
        "At a certain point the slope becomes zero. That means we reached a minimum/maximum value, because the function stopped changing. For our specific function, this point is $x=2/3$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMQo3Xgv6M2N",
        "outputId": "1112ebfd-d947-4122-84f2-4d6788198094"
      },
      "outputs": [],
      "source": [
        "x = 2/3\n",
        "print(f\"f(x) value is about zero: {(f(x + h) - f(x))/h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87LGCHhiCVI"
      },
      "source": [
        "#### Derivative of a function with multiple inputs [00:14:12](https://www.youtube.com/watch?v=VMj-3S1tku0&t=852s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvHtFgOVTiY5"
      },
      "source": [
        "Now let's take a little more complex case. We now have a function of three scalar inputs: `a`, `b`, and `c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPQyITPv3pfU"
      },
      "outputs": [],
      "source": [
        "h = 0.0001\n",
        "\n",
        "def d(a,b,c):\n",
        "  return a*b+c\n",
        "\n",
        "# inputs\n",
        "a = 2.0\n",
        "b = -3.0\n",
        "c = 10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySuANdqeVKWr"
      },
      "source": [
        "What happens to our function $f(x) = a*b+c$, given $a = 2,\\ b=-3,\\ c= 10$ if we bump `a` a tiny bit?\n",
        "\n",
        ".\n",
        ".\n",
        "\n",
        "\n",
        "Intuitively, this amplifies the negative component, `b`, so function value should go down. That would make the slope negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx455FQdVJjy",
        "outputId": "04df2946-462f-467b-ce21-32eb48aaa8fd"
      },
      "outputs": [],
      "source": [
        "d1 = d(a, b, c)\n",
        "d2 = d(a+h, b, c)\n",
        "\n",
        "print(d1)\n",
        "print(d2)\n",
        "print(f\"Slope: {(d2-d1)/h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HZ8JpNWxkF"
      },
      "source": [
        "Now what if we bump `b` a tiny bit?\n",
        "\n",
        ".\n",
        ".\n",
        "\n",
        "This would reduce the negative contribution to the function value, spelling a value increase. Then the slope of tangent line at that point would be positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMb2L_4lfFE7",
        "outputId": "19b0b90e-76c9-4ff3-dfe4-09849cd64d8a"
      },
      "outputs": [],
      "source": [
        "d1 = d(a, b, c)\n",
        "d2 = d(a, b+h, c)\n",
        "print(f\"Slope: {(d2-d1)/h}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_2FEsPfTrw"
      },
      "source": [
        "What would changing `c` do?\n",
        "\n",
        ".\n",
        ".\n",
        "\n",
        "Plugging in the numbers mentally, we see that it should increase function value a bit. The change would be rather small, because we're just adding, not multiplying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Jrtf-ufKmt",
        "outputId": "63304246-470a-42b7-d59b-9d6857dc464f"
      },
      "outputs": [],
      "source": [
        "d1 = d(a, b, c)\n",
        "d2 = d(a, b, c+h)\n",
        "print(f\"Slope: {(d2-d1)/h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvEwH2qagcC5"
      },
      "source": [
        "_**Fun fact**_: We just learned the intuitions used by Isaac Newton and Gottfried Leibniz back in the 17th century. Newton did this because he needed a tool for describing movement of bodies in the sky and on Earth, while Leibnitz was more concerned with pure mathematics and studied geometry of change. For more, take a look at _The Math Book: Big ideas simply explained, p. 268_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFXmI-YkEPgM"
      },
      "source": [
        "## So what?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnFrsBTREX_g"
      },
      "source": [
        "- You saw how the derivative gives you the direction and scale of change in function value, as we plug in different input values.\n",
        "\n",
        "- As we'll see soon, that's one of the key things allowing neural nets to learn. They do so by gradually finding parameter values that nudge the _error function_ towards zero.  \n",
        "\n",
        "- So far, we used functions of up to three parameters. Large neural networks use _billions_. But this key intuition remains the same.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln1n6Nw8PFkw"
      },
      "source": [
        "# Building the Value class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzwDno5BFoQ7"
      },
      "source": [
        "A neural network is essentially a mathematical expression. It takes in (1) input data and (2) parameters to output (3) predictions. We are now going to build the basic building block of such expression - the `Value` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fasHuA8diIPc"
      },
      "source": [
        "#### Starting the core Value object of micrograd and its visualization [00:19:09](https://www.youtube.com/watch?v=VMj-3S1tku0&t=1149s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbKWmD3IF8OS"
      },
      "source": [
        "`Value` is a simple scalar wrapper serving as an atom of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lesm1xCgJ4Q"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#/Java\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  # lets us do a + b with Value objects\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    return out\n",
        "\n",
        "  # lets us do a * b with Value objects\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    return out\n",
        "\n",
        "\n",
        "  # tanh activation function\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), _op='tanh')\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CdasDtzKKtD"
      },
      "source": [
        "Note how we can now perform arithmetic on `Value` objects and assign labels to each instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZqwm9S3KtXa"
      },
      "outputs": [],
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label = 'b')\n",
        "c = Value(10.0, label='c')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6BUWGVybvm6"
      },
      "source": [
        "Under the hood `a + b` calls `Value.__add__`. Similarly `a*b` relies on `Value.__mul__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCkoxd5Hbuei",
        "outputId": "6a86df64-8e53-4396-c9ea-0a08ece0330a"
      },
      "outputs": [],
      "source": [
        "print(a+b)\n",
        "print(a.__add__(b))\n",
        "print(Value.__add__(a, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vavMmzpKdiOe"
      },
      "source": [
        "Note how `Value` constructor accepts child node list as an argument:\n",
        "\n",
        "```python\n",
        "def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aRi-dblwXe"
      },
      "source": [
        "This argument is set during arithmetic operations, such as `__add__`:\n",
        "\n",
        "```python\n",
        "  # lets us do a + b with Value objects\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    return out\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AtLrCXjJgmp"
      },
      "source": [
        "This design lets us visualize `L` as a computation graph, showing how it evolved in a series of arithmetic operations with other `Value` objects. This is what we're doing next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX317BG3n7Jv"
      },
      "source": [
        "#### Visualizing the computation graph [24:58](https://youtu.be/VMj-3S1tku0?t=1498)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3rDwTnro1y0"
      },
      "source": [
        "The slightly scarry visualization code below is not essential to the subject of this lecture (_but the plot it produces is!_). Basically, it takes a `Value` object, traverses all of its children and uses  `graphviz` open-source graph visualization library to plot the nodes and operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNdGtaK-IuaH"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in sorted(nodes, key=lambda x:x.label):\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this node to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijTWi5rt6H-t"
      },
      "source": [
        "Time for some magic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Yk48Hl9brR",
        "outputId": "9a2efa48-09b2-40c0-ee15-3afdccb7abb4"
      },
      "outputs": [],
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label = 'b')\n",
        "c = Value(10.0, label='c')\n",
        "\n",
        "e = a*b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "\n",
        "L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "A_jQJrt3JxAN",
        "outputId": "95c8bca8-7570-4e1a-a19e-a564f83c86cd"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-eIuXgV5LQQ"
      },
      "source": [
        "> **Implementation note:** This code uses round \"fake graph nodes\" to display operations like `add` and `multiply`. The are not `Value` objects. The only real `Value` objects are the things in squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCAPb0IxI2Tb"
      },
      "source": [
        "This graph gives us an intuition into what neural networks look like under the hood. They are massive mathematical expression trees that resemble this one on a huge scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cODHbQRq6PzB"
      },
      "source": [
        "### Interim recap [29:02](https://youtu.be/VMj-3S1tku0?t=1742)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jQ5Zb26qE2"
      },
      "source": [
        "Where we are so far:\n",
        "\n",
        "- We are able to build out mathematical expressions using `+ and *`;\n",
        "- We can do a single _forward pass_ through a mathematical expression, going from initial inputs all the way to the single output node `L`;\n",
        "- We can visualize `L` as a computation graph.\n",
        "\n",
        "Next, we are going to learn how to do _backpropagation_. This is an important step in training a neural net. It involves going through the computation graph backwards from `L`, figuring out the derivative of `L` with respect to every relevant _leaf node_. That gives you individual node contributions to `L`: do they cause its value to increase or decrease and how rapidly.\n",
        "\n",
        "In neural net setting, output node `L` is typically referred to as the _loss function_, while other nodes may represent input data, trainable _weights_, or intermediate computations. Our goal is to gently nudge _trainable weights_ to a value that minimizes `L`, pushing it as close to zero as practically possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7lnhqN_WgQ2"
      },
      "source": [
        "# Manual backpropagation example #1 [00:32:10](https://www.youtube.com/watch?v=VMj-3S1tku0&t=1930s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyuSqq4_HX2j"
      },
      "source": [
        "Note how each `Value` node has a property called `grad`. It is there to hold the _gradient_ - the derivative of `L` with respect to that node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "LulssMVQYiBL",
        "outputId": "b85d91dc-7ead-423d-d66c-d575021ed59e"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c3LFEt1WofI"
      },
      "source": [
        "We will now go through the entire computation graph backwards, starting with node **L** and setting individual gradients along the way.\n",
        "\n",
        "What is the derivative of `L` with respect to `L`? In other words, if I change `L` by a tiny amount _h_, what is the impact of that change on `L`? :)\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "Intuitively, the impact is proportional and the derivative will be $1$. Now let's estimate that numerically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KspfmveKqks",
        "outputId": "5844ae72-5e8e-4618-856f-d43c935349c2"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def derivative_of_L():\n",
        "\n",
        "  h = 0.00001 # a tiny distance\n",
        "\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L1 = L.data\n",
        "  L2 = L.data + h\n",
        "\n",
        "  print((L2-L1)/h) # rise/run = rate of change\n",
        "\n",
        "derivative_of_L()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF-rpR8acos6"
      },
      "source": [
        "No surprise: we see that $dL$ is about $1$. So let's update our `L.grad` now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "CmkilxZLdBUX",
        "outputId": "d514ec15-352f-41a2-d8c1-62fcc4c96674"
      },
      "outputs": [],
      "source": [
        "L.grad = 1.0\n",
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNApzgaddLNH"
      },
      "source": [
        "### Computing the partial derivatives of `L` with respect to `d` and `f`  [34:40](https://youtu.be/VMj-3S1tku0?t=2081)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWorbn5defb"
      },
      "source": [
        "So how does changing `d` and `f` change `L`?\n",
        "\n",
        "Let's start with `d`. We know that `L = d * f` and we want to know what is $dL/dd$. If you know your calculus, the analytic solution is $dL/dd = f$. But we don't have to believe that. Let's derive it.\n",
        "\n",
        "1. By definition, a derivative can be computed as $f'(x) = \\large\\frac{(f(x + h) - f(x))}{h}$\n",
        "\n",
        "2. Our `x` is `d * f`, so: $dL/dd = \\large\\frac{(d+h) * f) - d*f}{h}$.\n",
        "\n",
        "3. Expanding that expression we get $dL/dd = \\large\\frac{d*f + h*f - d*f}{h}$ = $\\large\\frac{h*f}{h}$ = $f$.\n",
        "\n",
        "4. By symmetry, $dL/df = d$.\n",
        "\n",
        "Let's update the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "F25eHvtGdSJz",
        "outputId": "ecffe9f3-9ca1-4da7-bb03-8ffa7d4be30c"
      },
      "outputs": [],
      "source": [
        "d.grad = f.data\n",
        "f.grad = d.data\n",
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gogSA-I44Y"
      },
      "source": [
        "Is this correct? Let's double check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTx7uUwzI9Ig",
        "outputId": "3f99e971-e44f-43ab-e10e-3c28d58914d1"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def dL_by_df():\n",
        "\n",
        "  h = 0.00001\n",
        "\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "  L1 = L.data\n",
        "\n",
        "  f = Value(-2.0 + h, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L2 = L.data\n",
        "\n",
        "  print((L2-L1)/h)\n",
        "\n",
        "dL_by_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVDpo33a-L2C",
        "outputId": "3db20753-a573-45ec-e0cc-821d6c83e731"
      },
      "outputs": [],
      "source": [
        "def dL_by_dd():\n",
        "\n",
        "  h = 0.0001\n",
        "\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd';\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "  L1 = L.data\n",
        "\n",
        "  d.data += h\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L2 = L.data\n",
        "\n",
        "  print((L2-L1)/h)\n",
        "\n",
        "dL_by_dd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suFFNToALGT6"
      },
      "source": [
        "### **The Crux of Backpropagation**: computing the partial derivatives of `L` with respect to `c` and `e` [38:09](https://youtu.be/VMj-3S1tku0?t=2289)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "9eADqaTEY9aU",
        "outputId": "2ef83938-0a0a-4b43-e118-6589bbf7f655"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_RN44NZEMA"
      },
      "source": [
        "> **Alert:** This will be the most important node to understand. If you understand the gradient for this node, you understand all of backpropagation.\n",
        "\n",
        "Take another look at the graph. How sensitive is `L` to `c`? I.e. if we wiggle `c`, how much does this change `L` through `d`?\n",
        "\n",
        "We already know how `c` impacts `d` and how `d` impacts `L`. Intuitively, there should be a way to plug these facts together to figure out the impact of `c`. In fact, there _is_ such a way.\n",
        "\n",
        "Let's start with figuring out the value of `dd/dc`. In other words, how much `c` impacts `d`.\n",
        "\n",
        "- By definition, a derivative can be computed as $f'(x) = \\large\\frac{f(x + h) - f(x))}{h}$\n",
        "\n",
        "- Our `x` equals `c + e`, so: `dd/dc` = $\\large\\frac{(c+h) + e) - (c+e)}{h}$.\n",
        "\n",
        "- Expanding that expression we get `dd/dc` = $\\large\\frac{h}{h}=1$.\n",
        "\n",
        "- By symmetry, `dd/de` is also `1`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-rClFVFN47t"
      },
      "source": [
        "But is that _really_ correct? Let's once again verify numerically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDWfB03ELXEs",
        "outputId": "c11c6e95-a179-4312-8e3c-247ff4fb3227"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def dd_by_dc():\n",
        "\n",
        "  h = 0.00001\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  d1 = d.data\n",
        "\n",
        "  c = Value(10.0 + h, label='c')\n",
        "  d = e + c; d.label = 'd'\n",
        "\n",
        "  d2 = d.data\n",
        "\n",
        "  print((d2-d1)/h)\n",
        "\n",
        "dd_by_dc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI9UiEyBP1V_"
      },
      "source": [
        "That looks about right. We now know `dd/dc` and `dL/dd`. These are the _local_ derivatives that we want to tie together with the output of this graph. How do we put them together to figure out the impact of `c` and `d` on `L`, i.e. `dL/dc` and `dL/dd`?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "_And the answer is_: The chain rule from calculus. Have a look at the Wikipedia [chain rule article](https://en.wikipedia.org/wiki/Chain_rule) for a refresher:\n",
        "\n",
        "> If a variable $z$ depends on the variable $y$, which itself depends on the variable $x$ (that is, $y$ and $z$ are dependent variables), then $z$ depends on $x$ as well, via the intermediate variable $y$. In this case, the chain rule is expressed as:\n",
        ">\n",
        "> $\\displaystyle {\\frac {dz}{dx}}={\\frac {dz}{dy}}\\cdot {\\frac {dy}{dx}}$\n",
        "\n",
        "Intuitively, the chain rule states that knowing the instantaneous rate of change of $z$ relative to $y$ and that of $y$ relative to $x$ allows one to calculate the instantaneous rate of change of $z$ relative to $x$ as the product of the two rates of change:\n",
        "\n",
        "> As put by George F. Simmons: _\"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\"_\n",
        "\n",
        "What this basically tells us is that we multiply connected rates of change, _chaining them together_. So:\n",
        "\n",
        "- $\\large\\frac{dL}{dc} = \\frac{dL}{dd} * \\frac{dd}{dc} = -2 * 1 = -2$\n",
        "\n",
        "- By symmetry, $\\large\\frac{dL}{de} = \\frac{dL}{dd} * \\frac{dd}{de} = -2 * 1 = -2$\n",
        "\n",
        "Is that right? Once again, lets verify this numerically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpDqyprsQNDe",
        "outputId": "fdb8df45-0c0f-4e9f-9a7d-3d41a4a9fa26"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def dL_by_dc():\n",
        "\n",
        "  h = 0.00001\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "  L1 = L.data\n",
        "\n",
        "  c = Value(10.0 + h, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L2 = L.data\n",
        "\n",
        "  print((L2-L1)/h)\n",
        "\n",
        "dL_by_dc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycp3Ivjer-cq"
      },
      "source": [
        "Intuition: the plus node of the graph simply routes the gradient as-is, since a derivative of a sum expression is just `1.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "5J1FLvmasM3n",
        "outputId": "ddc86a6c-5b63-4c3d-efac-55dff899416d"
      },
      "outputs": [],
      "source": [
        "c.grad = -2.0 * 1.0\n",
        "e.grad = -2.0 * 1.0\n",
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQZZCCLu0vX"
      },
      "source": [
        "In our case, the distance between the _local derivative_ and computation graph output `L` is just one step. In general, this distance can be massive, but same _chaining_ idea applies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K1DIgr34AU5"
      },
      "source": [
        "### Computing the partial derivatives of `L` with respect to `a` and `b` [47:10](https://youtu.be/VMj-3S1tku0?t=2832)\n",
        "\n",
        "Now we are going to take another step back to figure out how `a` and `b` affect `L`.\n",
        "\n",
        "Let's start with `a`. Thanks to the chain rule, we only need to multiply `de/da` by `dL/de` to figure out the impact of `a`. And then apply the same principle to `b`. Let's formulate what we know and what we want:\n",
        "\n",
        "*KNOW*:\n",
        "- `dd/de = -2.0`\n",
        "- `de/da = b = -3.0 (multiplication rule from calculus. We derived it for L above.`\n",
        "\n",
        "*WANT*:\n",
        "- `dL/da = dd/de * de/da = -2.0 * -3.0 = 6.0 (chain rule)`\n",
        "\n",
        "Are we right? Numerical verification time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm44p50Z5A96",
        "outputId": "dbe20b64-caaa-4e77-f816-e8a02992ddf6"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def dL_by_da():\n",
        "\n",
        "  h = 0.00001\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "  L1 = L.data\n",
        "\n",
        "  a = Value(2.0 + h, label = 'a')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L2 = L.data\n",
        "\n",
        "  print((L2-L1)/h)\n",
        "\n",
        "dL_by_da()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE7wR7yx9EzX"
      },
      "source": [
        "To obtain `dL/db`, we apply the same principle:\n",
        "\n",
        "*KNOW*:\n",
        "- `dd/de = -2.0`\n",
        "- `de/db = a = 2.0 (multiplication rule from calculus. We derived it for L above.`\n",
        "\n",
        "*WANT*:\n",
        "- `dL/db = dd/de * de/db = -2.0 * 2.0 = -4.0 (chain rule)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AllJY2R79fyH",
        "outputId": "81e79950-3b32-4ae5-a457-7b70041e522c"
      },
      "outputs": [],
      "source": [
        "# encapsulating into a function\n",
        "# to avoid polluting the scope with scratchpad code.\n",
        "def dL_by_db():\n",
        "\n",
        "  h = 0.00001\n",
        "  a = Value(2.0, label='a')\n",
        "  b = Value(-3.0, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "  L1 = L.data\n",
        "\n",
        "  b = Value(-3.0 + h, label = 'b')\n",
        "  c = Value(10.0, label='c')\n",
        "  e = a*b; e.label = 'e'\n",
        "  d = e + c; d.label = 'd'\n",
        "  f = Value(-2.0, label='f')\n",
        "  L = d * f; L.label = 'L'\n",
        "\n",
        "  L2 = L.data\n",
        "\n",
        "  print((L2-L1)/h)\n",
        "\n",
        "dL_by_db()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-dUYXCx9vgw"
      },
      "source": [
        "We can now update the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "A-6_MxpG9x6i",
        "outputId": "36a74189-8ce7-4b4a-b9b8-b2768475a344"
      },
      "outputs": [],
      "source": [
        "a.grad = 6.0\n",
        "b.grad = -4.0\n",
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du0Yrmok-wwE"
      },
      "source": [
        "That's it. We did manual backpropagation through the graph, step-by-step, applying the derivative chain rule from calculus. This is the essence of backpropagation: it's the recursive application of the chain rule through the computation graph, starting from the final output node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBJAX1IFPQG"
      },
      "source": [
        "# Preview of a single optimization step [00:51:11](https://youtu.be/VMj-3S1tku0?t=3071)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-mQI6jSFTFO"
      },
      "source": [
        "So how do we use gradient values in practice?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "Say, our goal is to make `L` go up. Then at each optimization step we will use the gradient values to nudge `L` in the direction of the gradient by a small amount, called _learning rate_. This involves updating the individual leaf nodes of the computational graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "Qno35R9X995y",
        "outputId": "570c34c0-f82e-4be6-97d0-ce6997d6d13e"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "a.data += learning_rate * a.grad\n",
        "b.data += learning_rate * b.grad\n",
        "c.data += learning_rate * c.grad\n",
        "f.data += learning_rate * f.grad\n",
        "\n",
        "e = a * b; e.label = 'e'\n",
        "d = c + e; d.label = 'd'\n",
        "L = d * f; L.label = 'L'\n",
        "\n",
        "draw_dot(L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQRm5pgJbVA"
      },
      "source": [
        "# Manual backpropagation example #2: a neuron [00:52:52](https://youtu.be/VMj-3S1tku0?t=3182)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVXjoChaF9YO"
      },
      "source": [
        "Let's take a look at a simplified neuron model, as taught in _Stanford CS231n_. We are going to backpropagate through a _neuron_. Biological neurons are complicated devices and what we have here is a very simple mathematical model losely based on a neuron - _perceptron_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ZW2eTCvQJmSj",
        "outputId": "5f4633f4-cbaf-4f07-cd6a-6ec9818a597b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://cs231n.github.io/assets/nn1/neuron_model.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpbaa9vKG0hE"
      },
      "source": [
        "What basically happens in a neural network is that we:\n",
        "\n",
        "1. feed in a bunch of inputs _x(i)_,\n",
        "2. multiply each input by its corresponding weight _w(i)_;\n",
        "3. Sum all the input-weight products and add a _bias_ term _b_\n",
        "4. Feed that through an activation function\n",
        "\n",
        "> - Weights _w_ are like _synaptic strength_ for each input;\n",
        "\n",
        "> - The _bias_ term _b_ can be viewed as a _happiness_ term. It makes a neuron less or more excited, regardless of the input value. Everyday analogy: Optimists sometimes can exaggerate even the smallest good news, while pessimists are prone to catastrophisizing insignificant events.\n",
        "\n",
        "> - The activation function squashes neuron output to a narrower range. That's useful for numerical stability during optimization.\n",
        "\n",
        "Out of the neuron comes the activation function applied to the dot product of the weights and inputs. That's all there is to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raN_bY1hJgSJ"
      },
      "source": [
        "Let's take a look at one popular activation function, the `tanh`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "lTj8W7DkH310",
        "outputId": "12667789-326a-449b-c369-32e8b92c6d67"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5,5,0.2)));\n",
        "plt.grid();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHuc3OUELL53"
      },
      "source": [
        "We see that `tanh` basically squashes `y(x)` to the range `[-1;1]`: If we input a very large value, `tanh` will cap it at one and very small values will be capped at `-1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50RARY8sT0C"
      },
      "source": [
        "#### Defining a toy neural net [55:20](https://youtu.be/VMj-3S1tku0?t=3320)\n",
        "\n",
        "We take two inputs `x1` and `x2`, two corresponding weights, a bias term and `tanh` as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nmqlW2GLEbg"
      },
      "outputs": [],
      "source": [
        "# inputs x1, x2\n",
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "\n",
        "# bias of the neuron\n",
        "b = Value(6.7, label='b')\n",
        "\n",
        "# Forward pass: multiply inputs and weights, add bias.\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IjdIKG6N1yz"
      },
      "source": [
        "This gives us the raw activation values without the activation function. We can now plot this mathematical expression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5LoyKWA4vD5p",
        "outputId": "563c1033-1f27-43af-8d59-5c3e01b5fd13"
      },
      "outputs": [],
      "source": [
        "draw_dot(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSQKUS8xdgP"
      },
      "source": [
        "We'll also need to implement the `tanh` activation function. There are several ways to compute it (look up on Wikipedia), we're going to use the exponentiation-based one and put it into our `Value` class:\n",
        "\n",
        "```\n",
        "def tanh(self):\n",
        "  n = self.data\n",
        "  tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "  out = Value(tanh, (self,), label='tanh')\n",
        "\n",
        "  return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPcTHrFDPhxf"
      },
      "source": [
        "Note that exponentiation is baked into `tanh` and not implemented as an atomic `Value` function. That's a deliberate design choice: we can pick the granularity we like, as long as we know how to backpropagate through the operation (_we'll get into that shortly!_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "r4cypCcpyQU6",
        "outputId": "4c2806c5-c369-4dcb-b5c0-9a4b7dbb7a98"
      },
      "outputs": [],
      "source": [
        "# inputs x1, x2\n",
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "\n",
        "# bias of the neuron\n",
        "b = Value(6.7, label='b')\n",
        "\n",
        "# Forward pass: multiply inputs and weights, add bias.\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "# activation function\n",
        "o = n.tanh();\n",
        "\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsC0dCH0kXv"
      },
      "source": [
        "Now let's have a look at `tanh` squashing things. What happens if we set `b` to a large value?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "EkIGWvFxzEng",
        "outputId": "eb8f650e-f872-4a63-997c-1992d2fb5dec"
      },
      "outputs": [],
      "source": [
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "b = Value(111, label='b')\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.tanh();\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Kl1ewA00BR"
      },
      "source": [
        "Although raw neuron output is huge at `101`, `tanh` squashes it to `1`, thus capping neuron _excitement_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BeyFUD33D8-"
      },
      "source": [
        "#### Manual backpropagation [01:01:04](https://youtu.be/VMj-3S1tku0?t=3662)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG4vAQjw6XRH"
      },
      "source": [
        "Resetting `b` to 6.8813735870195432 as Andrey promised it will give us nice numbers. ¯\\_(ツ)_/¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Nn863FmU54uq",
        "outputId": "5f7c8406-da71-42cd-913b-ac94df141493"
      },
      "outputs": [],
      "source": [
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.tanh(); o.label = 'o' # 'o' is short for 'output' here\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7oudlFDrwea"
      },
      "source": [
        "Now we are going to go through all the nodes starting with `o` and fill all the gradients. A real-life neural network would be huge and you can think of the neuron here as of a small piece of a large puzzle. We would be most interested in figuring out how weight values $w_i$ affect output and would tweak them to make output as small as we can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMfiwZKc3ECY"
      },
      "source": [
        "First, we compute the derivative of `o` with respect to itself. As we know from before, this is always `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "st_tz8R60w8-",
        "outputId": "1566dbe3-d787-4424-c499-d65c997f0b1c"
      },
      "outputs": [],
      "source": [
        "o.grad = 1.0\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWZoQdDD3ZYR"
      },
      "source": [
        "To figure out how much `n` impacts `o`, we compute `do/dn`. Wikipedia tells us that one way to compute the derivative of `tanh(x)` is $1-tanh(x)^2$. Applying that to our case:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol8-oUsj3W-9"
      },
      "outputs": [],
      "source": [
        "n.grad = 1.0 - o.data**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tigPOL_f5Vk9",
        "outputId": "f5ae2eaf-1469-4bf0-fae7-b7ad79ca9cc4"
      },
      "outputs": [],
      "source": [
        "n.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "KIHgyCDXuJIu",
        "outputId": "dd8aa394-2ed1-4515-9ba0-72a38bd57fd9"
      },
      "outputs": [],
      "source": [
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GFkCRdA7uGd"
      },
      "source": [
        "Going one step back, we have a plus node. From our first backpropagation example, we know that gradient just flows through the plus node as-is. Thus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dos8jCbH6Jl8"
      },
      "outputs": [],
      "source": [
        "x1w1x2w2.grad = 1.0 * n.grad\n",
        "b.grad = 1.0 * n.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "VHqVjqDDuSHO",
        "outputId": "56a282a5-158e-4ab4-a5f5-071a9aa3698b"
      },
      "outputs": [],
      "source": [
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxTKSR2R9J9b"
      },
      "source": [
        "Next, we've got yet another plus node and the same principle applies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHqtrV3t8SeF"
      },
      "outputs": [],
      "source": [
        "x2w2.grad = 1.0 * x1w1x2w2.grad\n",
        "x1w1.grad = 1.0 * x1w1x2w2.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "REsm3ir_uogX",
        "outputId": "8d1aa789-8ffe-442e-c0f5-6bf92c93f8d4"
      },
      "outputs": [],
      "source": [
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6KoLCtb9xTt"
      },
      "source": [
        "Finally, we arrived at two multiplication nodes feeding weights and raw data into the network. From our first manual backprop example we know that the local derivative of a _times_ node is _the other term_. Hence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMBrg9rB9XMq"
      },
      "outputs": [],
      "source": [
        "x2.grad = w2.data * x2w2.grad # local piece of chain rule\n",
        "w2.grad = x2.data * x2w2.grad\n",
        "\n",
        "x1.grad = w1.data * x1w1.grad\n",
        "w1.grad = x1.data * x1w1.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB5SlF_n_TbI"
      },
      "source": [
        "We're done: we've backpropagated through the entire graph. Let's redraw:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "RhNTXCaH-kmc",
        "outputId": "b1378eac-447a-45d5-8624-36f66aa33b86"
      },
      "outputs": [],
      "source": [
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbODJBEfG2Sv"
      },
      "source": [
        "# Automating backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5UPuVH6AWqQ"
      },
      "source": [
        "Doing backprop by hand is obviously suboptimal once we've learned how it works. Let's update our `Value` class so that it backpropagates automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZbswYZvAUu5"
      },
      "source": [
        "## Implementing the backward function for each operation [01:09:02](https://youtu.be/VMj-3S1tku0?t=4142)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4q5yoB_W6qV"
      },
      "source": [
        "Since the exact mechanics of computing a local gradient depend on the operation, we'll redefine the `add`/`mul`/`tanh` methods to apply the right kind of logic on the `Value` instance. All the logic directly follows from the calculus rules we used when doing backpropagation manually.\n",
        "\n",
        "The newly introduced `_backward` function will specify how to chain the outputs gradients into the inputs gradients. By default it does nothing. However, if the `Value` object is being created through a mathematical operation, specifics of applying the local chain rule will be passed along:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5uikouXAS9V"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      self.grad = other.data * out.grad\n",
        "      other.grad = self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      local_derivative = (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc8BVch9ZqvO"
      },
      "outputs": [],
      "source": [
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.tanh();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "XWp591uTZ-lD",
        "outputId": "03f171f7-1630-4dbd-f43a-52abf10db6ef"
      },
      "outputs": [],
      "source": [
        "o.grad = 1.0\n",
        "nodes = [o,n,b,x1w1x2w2,x1w1,x2w2]\n",
        "for node in nodes:\n",
        "  node._backward()\n",
        "\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4S3smzmnLMC"
      },
      "source": [
        "## Implementing the backward function for a whole expression graph [01:17:32](https://www.youtube.com/watch?v=VMj-3S1tku0&t=4652s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADUk1qmCnhgF"
      },
      "source": [
        "We implemented backpropagation for each individual node. However, calling the `_backward` method manually on each node isn't practical. Ideally, we want to do something like `o._backward` and have gradients automatically backpropagate all the way to the left of the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qN5GGaogzg"
      },
      "source": [
        "A node is ready to backpropagate only once all of its descendants have their gradients computed. This means we need a way of ordering nodes. This problem is called _topological sorting_:\n",
        "\n",
        "> Precisely, a _topological sort_ is a graph traversal in which each node $v$ is visited only after all its dependencies are visited. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a _directed acyclic graph (DAG)_.\n",
        "> \\-- [Wikipedia](https://en.wikipedia.org/wiki/Topological_sorting#:~:text=Precisely%2C%20a%20topological%20sort%20is,directed%20acyclic%20graph%20(DAG))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vpu313K3y9"
      },
      "source": [
        "Note how the code below recursively adds node children and only then the current node itself. This gives us a left-to-right list of our graph nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaR3snqUafDP",
        "outputId": "a8c2dc8c-95f1-4c58-b3fb-77998c791680"
      },
      "outputs": [],
      "source": [
        "topo = []\n",
        "visited = set()\n",
        "def build_topo(v):\n",
        "  if v not in visited:\n",
        "    visited.add(v)\n",
        "    for child in v._prev:\n",
        "      build_topo(child)\n",
        "    topo.append(v)\n",
        "\n",
        "build_topo(o)\n",
        "topo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-7iYXngw75b"
      },
      "source": [
        "Now lets place this into our `Value` class. We're going to define a public `backward` method (note no underscore) with this logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBLi8CPYxNkH"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      self.grad = other.data * out.grad\n",
        "      other.grad = self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      local_derivative = (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    # we need to go in reversed order - from right to left of the graph.\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZIFPWhky92R"
      },
      "source": [
        "Now let's reset the graph:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af1ARp7yyiy5"
      },
      "outputs": [],
      "source": [
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.tanh();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpH92F3azC_7"
      },
      "source": [
        "And see if `backpropagate` works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "PUwOJLgszI8v",
        "outputId": "9b7351d4-9355-406d-e580-3560812cb660"
      },
      "outputs": [],
      "source": [
        "o.backward()\n",
        "draw_dot(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbZ0FgrQ0BTm"
      },
      "source": [
        "## Fixing a backprop bug when one node is used multiple times [01:22:30](https://www.youtube.com/watch?v=VMj-3S1tku0&t=4952s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5z2RiuW0ozq"
      },
      "source": [
        "There's a problem with our code. Any time a `Value` is used more than once, the gradient will be incorrect. Here's a demo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "mhWmoDTdzZo3",
        "outputId": "74362823-3ac8-45c2-9172-290909b794e7"
      },
      "outputs": [],
      "source": [
        "a = Value(-2.0, label = 'a')\n",
        "b = Value(3.0, label = 'b')\n",
        "\n",
        "d = a * b; d.label = 'd'\n",
        "e = a + b; e.label = 'e'\n",
        "f = d * e; f.label = 'f'\n",
        "\n",
        "f.backward()\n",
        "draw_dot(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aPGazrN1jUS"
      },
      "source": [
        "Gradients of `a` and `b` are wrong, because they are being overwritten during backpropagation from `d` and `e`. How do we fix that?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "The fix is to accumulate gradients instead of setting them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aMXyvJd62ir"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      local_derivative += (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "mKjppVoW7Mwl",
        "outputId": "52a03671-0574-466d-a11e-6a21affa8233"
      },
      "outputs": [],
      "source": [
        "a = Value(-2.0, label = 'a')\n",
        "b = Value(3.0, label = 'b')\n",
        "d = a * b; d.label = 'd'\n",
        "e = a + b; e.label = 'e'\n",
        "f = d * e; f.label = 'f'\n",
        "\n",
        "f.backward()\n",
        "draw_dot(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uB6MoZB70uR"
      },
      "source": [
        "#  Breaking up `tanh`, exercising with more operations [01:27:05](https://www.youtube.com/watch?v=VMj-3S1tku0&t=5225s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBgxqbKIsmtN"
      },
      "source": [
        "Right now code like this won't work because of value conversion:\n",
        "\n",
        "```\n",
        "a = Value(1)\n",
        "b = a + 2\n",
        "```\n",
        "\n",
        "To the interpreter this looks like adding two different types. It doesn't know how to do that, unless we state it.\n",
        "\n",
        "Another case is reverse operand ordering:\n",
        "\n",
        "```\n",
        "a = Value(1)\n",
        "b = 2 * a\n",
        "```\n",
        "\n",
        "In this case Python will attempt to call 2.`__mul__(a)` and will fail, since this type handling isn't implemented. To work around this, there's a built-in method called `__rmul__`, which is called as a backup whenever `__mul__` fails. `__rmul__` basically is a way to ask `a` if it knows how to multiply by 2.  \n",
        "\n",
        "As a matter of convenience, let's improve implementation of arithmetic operations in our `Value` class to cover these cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhlavY457Nyi"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      local_derivative = (1-tanh**2)\n",
        "      self.grad += local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkA9qWNcCx9T"
      },
      "source": [
        "Now let's implement exponentiation, complete with backpropagation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nNb9E3NC4Tj"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      local_derivative += (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self, ), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      local_derivative = 1.0 if out.data == 0 else out.data\n",
        "      self.grad += local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4sNmsulCXxu",
        "outputId": "0fa2e9ed-fa83-44b5-e9eb-604985c0eea4"
      },
      "outputs": [],
      "source": [
        "a = Value(2.0)\n",
        "a.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFFNw96SDIY6"
      },
      "source": [
        "Finally, let's have a look at implementing division. Consider the following case:\n",
        "\n",
        "```\n",
        "a = Value(2.0)\n",
        "b = Value(4.0)\n",
        "a / b # We want to get `0.5` as the result\n",
        "```\n",
        "\n",
        "One neat trick is to redefine division as multiplication:\n",
        "\n",
        "```\n",
        "a / b = a * 1/b = a * b^-1\n",
        "```\n",
        "\n",
        "This is useful because it reduces the amount of floating point rounding errors  when dealing with a large number of repetitive division calculations. Multiplication is also faster than division.\n",
        "\n",
        "> Note: Modern compilers and interpreters can automatically rewrite division as multiplication if they deem that helpful. Therefore, it's not a silver bullet rule. In this specific case, it's a useful excercise. In general, profilers and benchmarks are your best friend for figuring out if this is worth doing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAQI8ghyHLjf"
      },
      "source": [
        "We're going to implement division in three parts:\n",
        "\n",
        "1. Use the `__truediv__` method to divide integers and floating point numbers by using multiplication and raising to negative power.\n",
        "\n",
        "2. Implement the `pow` method to raise a given number to the power we want and have `__truediv__` call it.\n",
        "\n",
        "3. Implement negation and substraction - we'll need them to compute the derivative of a power function.\n",
        "\n",
        "> Note: When you use the division operator (/) between two objects, Python internally calls the `__truediv__` method of the left operand object and passes the right operand object as an argument. The `__truediv__` method should return the result of the division."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-dq6teSDHkz"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __neg__(self):\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)),\\\n",
        "     \"only supporting int/float powers for now\"\n",
        "\n",
        "    out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * self.data ** (other-1) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    # When you use the division operator (/) between two objects,\n",
        "    # Python internally calls the __truediv__ method\n",
        "    # of the left operand object and passes the right operand\n",
        "    # object as an argument.\n",
        "    # The __truediv__ method should return the result of the division.\n",
        "    return self * other**-1\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      local_derivative = 0.0\n",
        "      local_derivative += (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self, ), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      local_derivative = 1.0 if out.data == 0 else out.data\n",
        "      self.grad += local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1TXiSJBP7wq"
      },
      "outputs": [],
      "source": [
        "a = Value(2.0)\n",
        "b = Value(4.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwGxTL_HP3Hx",
        "outputId": "faaf7ce0-f4ab-4807-8c16-75a177c661c1"
      },
      "outputs": [],
      "source": [
        "a / b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdop06yP_Jo",
        "outputId": "b3d6a708-c06b-4cc9-8ce9-d50b1623dff0"
      },
      "outputs": [],
      "source": [
        "a - b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "N_pfKsddQYPG",
        "outputId": "c0f15728-987e-407f-d04f-56c2c6c78610"
      },
      "outputs": [],
      "source": [
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "\n",
        "# Redefine `tanh` through a mathematically equivalent\n",
        "# combination of atomic mathematical operations we've just added.\n",
        "# o = n.tanh()\n",
        "# exp-based formula for `tanh` (see Wikipedia)\n",
        "e = (2*n).exp()\n",
        "o = (e - 1) / (e + 1)\n",
        "o.label = 'o'\n",
        "o.backward()\n",
        "draw_dot(o)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU-tpeVlT23j"
      },
      "source": [
        "The takeaway of this chapter:  We saw how we could add and change operations and, as long as they're (a) mathematically equivalent and (b) implement the local backward/forward propagation pass correctly, this new logic is plugged into the graph automatically. The chain rule lets you focus on just one step at a time, no matter how complex the entire graph is.\n",
        "\n",
        "The granularity level at which you implement operations is totally up to you. You can implement atomic or coarse operations, as with `tanh`. The only thing that matters is being able to correctly compute the local derivative and apply the chain rule from calculus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3TIvLfsg0Gp"
      },
      "source": [
        "# Doing the same thing but in PyTorch: comparison [01:39:31](https://www.youtube.com/watch?v=VMj-3S1tku0&t=5972s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA__9GPfa8Ll"
      },
      "source": [
        "Now we'll see how to implement the same thing using `PyTorch` - a modern deep learning library that you would likely use in production. `micrograd` is roughly modeled after `PyTorch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NXFWVdeEJMH"
      },
      "source": [
        "PyTorch uses a `Tensor` type for representing data. Semantically it's similar to our `Value` type. However, `Tensor` is basically a multidimensional scalar array, while our `Value` class holds just one scalar number at a time.\n",
        "\n",
        "Some subtleties to note:\n",
        "\n",
        "1. PyTorch uses `float32` for scalar representation by default. If you'd like to represent scalars with double precision (like Python does by default), use `Tensor.double()`\n",
        "\n",
        "2. PyTorch `Tensor` is much more efficient than our basic `Value` class and supports parallel calculations.\n",
        "\n",
        "3. Gradient computation is off by default for efficiency reasons. To turn it on, you need to use `Tensor.requires_grad = True`\n",
        "\n",
        "You can also take a look at how various operations are implemented in the [Tensor source code](https://github.com/pytorch/pytorch/blob/main/torch/_tensor.py)\n",
        "\n",
        "Have a look at sample code below. It implements the same network we've built manually in PyTorch. The API is pretty much the same. And gradient computation yields the same results we obtained in previous steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etKNPLmnJPwu",
        "outputId": "34c6d3f9-a355-4eff-bb38-8a0e5f3e825c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.Tensor([2.0]).dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9PvyG0Wg3uJ",
        "outputId": "6096d4f4-4907-40bd-f93b-e0ff8d530306"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True\n",
        "x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True\n",
        "w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\n",
        "w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True\n",
        "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n",
        "\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.tanh(n)\n",
        "\n",
        "print(o.data.item())\n",
        "o.backward()\n",
        "\n",
        "print('---')\n",
        "print('x1:', x1.grad.item())\n",
        "print('w1:', w1.grad.item())\n",
        "print('x2:', x2.grad.item())\n",
        "print('w2:', w2.grad.item())\n",
        "print('b:', b.grad.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiRBGnLHcHc_"
      },
      "source": [
        "Note how `o` has a `backward` function, just like in our implementation of the `Value` class. We can pop out `grad` values too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiQaDzt8caAP",
        "outputId": "03cb786d-2782-49c4-ee81-429fe51dd7b6"
      },
      "outputs": [],
      "source": [
        "x1.grad.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI35umZWcp7m"
      },
      "source": [
        "`PyTorch` can do everything we implemented in the `Value` class and more. While `micrograd` handles a special case where the values are single scalar numbers, `PyTorch` works with n-dimensional tensors and is significantly more computationally efficient, allowing to run lots of operations in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq5xgR86lZUt"
      },
      "source": [
        "# Building out a neural net library (multi-layer perceptron) in micrograd [01:43:55](https://www.youtube.com/watch?v=VMj-3S1tku0&t=6235s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50SBTVBIlnBB"
      },
      "source": [
        "Next we'll build a neural net piece by piece. We'll end up with a two-layer _multi-layer perceptron (MLP)_. As mentioned, neural nets are just a specific class of mathematical expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLZmgZpE9TZG"
      },
      "source": [
        "Let's start by implementing a single neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "tdhRIC6mH8PV",
        "outputId": "3b2ec224-c44d-49cd-f6ed-7789759bc785"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://cs231n.github.io/assets/nn1/neuron_model.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3neUXcLAxZQ"
      },
      "source": [
        "Note that the activation value will be different at each run, since we're initializing weights to random values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THQ5c0tu9kQw",
        "outputId": "1ed414a0-657a-45a0-bec5-1872e5331475"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "  def __init__(self, nin):\n",
        "    \"\"\"\n",
        "      nin: number of inputs\n",
        "    \"\"\"\n",
        "    self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1, 1))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # w * x + b\n",
        "    # zip iterates over pairwise tuples obtained by pairing two sequences\n",
        "    activation = sum([wi*xi for wi, xi in zip(self.w, x)], self.b)\n",
        "    out = activation.tanh()\n",
        "\n",
        "    return out\n",
        "\n",
        "x = [2.0, 3.0]\n",
        "n = Neuron(2)\n",
        "n(x) #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKcnABaaLz7q"
      },
      "source": [
        "Next, let's define a layer - basically, a logical group of individual neurons that are not connected to each other and evaluated independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVvWXtGS_zP_",
        "outputId": "37491d88-a15a-404d-a09d-9a4b7a64109a"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    # as a matter of convenience, unwrap the value\n",
        "    # out of array if it's just a single value.\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "x = [2.0, 3.0]\n",
        "n = Layer(2, 3)\n",
        "n(x) #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFSGF2iyOBdA"
      },
      "source": [
        "Finally, let's complete the picture and define an MLP. As the image below shows, an MLP is just a sequence of fully interconnected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "GimyoDLtN_2Z",
        "outputId": "51e81c26-cecb-48dc-a0f0-80c790b0b566"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://cs231n.github.io/assets/nn1/neural_net2.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJf17kIXNtX6",
        "outputId": "92944bea-bb14-4871-a0b6-eeb9def1ea82"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self, nin, nouts):\n",
        "    sizes = [nin] + nouts\n",
        "    self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "x = [2.0, 3.0, -1.0]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "n(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epT0OQLiruoa"
      },
      "source": [
        "Finally, we can draw the computational graph of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1shwWs0vQCPQ",
        "outputId": "659e942a-5fb4-4ca1-ab7a-d341738dbeef"
      },
      "outputs": [],
      "source": [
        "mlp = n(x)\n",
        "mlp.grad = 1.0\n",
        "mlp.backward()\n",
        "draw_dot(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvYOUuYUHVX0"
      },
      "source": [
        "Check out how much more complex it gets, compared to what we started with. Wouldn't want to backpropagate over that with pen and paper! With `micrograd` we will be able to backpropagate through the entire expression and into the weights of the neurons. Let's see how that works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsK0DQYo3xBD"
      },
      "source": [
        "## Creating a tiny dataset, writing the loss function [01:51:04](https://www.youtube.com/watch?v=VMj-3S1tku0&t=6664s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VVntr7E5_sv"
      },
      "source": [
        "How does a neural network learn? It basically minimizes an error function (a.k.a. _loss function_), using the gradients to tweak neuron weights. We've built pretty much all the infrastructure to go through a small example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc4x-SV16sAs"
      },
      "source": [
        "Let's start with the dataset. We define a set of inputs `xs` and a set of desired outputs `ys` with the goal of having our network output the correct `y`. It's basically a _binary classifier_ example: given an input feature vector, we want to categorize it into one of the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ksf982Qrr7K",
        "outputId": "10a07a45-2ab6-4c4c-91a1-6046e139d5bd"
      },
      "outputs": [],
      "source": [
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0],\n",
        "]\n",
        "\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "ypred = [n(x) for x in xs]\n",
        "ypred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofqqFWXt8QGL"
      },
      "source": [
        "How do we make our neural net predict numbers that are closer to expected values?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "The trick is to define a single number that measures how well the entire network is performing. This is what the _loss function_ does. It tells us how _wrong_ the entire network is in making predictions. We want to make the network _less wrong_, ideally bringing loss function value down to zero.\n",
        "\n",
        "There are different varieties of loss functions. We'll use the _Mean Squared Error (MSE)_ loss.\n",
        "\n",
        "> **_MSE computational recipe_:** for each sample calculate how far a prediction `yout` is from the ground truth `ygt`, square that number to make sure it's always positive, sum individual errors over all samples to get the loss value for the entire network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "RdBm6rCL7HNZ",
        "outputId": "69a1f250-9b40-4f5a-f727-74830bd7035f"
      },
      "outputs": [],
      "source": [
        "loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcuxPFkIdXlk"
      },
      "source": [
        "_Whoops, that's an error!_ Looks like we'll need to update the `Value` class to support reflected operand order for the `+` operation, just as we did with multiplication earlier in this tutorial.\n",
        "\n",
        "> _**Note**_: `__radd__` method is one of the special methods that can be defined in a class to emulate numeric objects. It is called to implement the binary arithmetic operations (+) with reflected (swapped) operands. This method is only called if the left operand does not support the corresponding operation and the operands are of different types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZIVLA5cwNe"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, data, children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0 # node gradient with respect to the loss function\n",
        "    self._backward = lambda: None # the backpropagation delegate\n",
        "    self._prev = set(children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  # This works akin to object.ToString() in C#\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "\n",
        "  def __neg__(self):\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)),\\\n",
        "     \"only supporting int/float powers for now\"\n",
        "\n",
        "    out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * self.data ** (other-1) * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    # When you use the division operator (/) between two objects,\n",
        "    # Python internally calls the __truediv__ method\n",
        "    # of the left operand object and passes the right operand\n",
        "    # object as an argument.\n",
        "    # The __truediv__ method should return the result of the division.\n",
        "    return self * other**-1\n",
        "\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    tanh = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(tanh, (self,), label='tanh')\n",
        "    def _backward():\n",
        "      # gradient accumulation\n",
        "      local_derivative = 0.0\n",
        "      local_derivative += (1-tanh**2)\n",
        "      self.grad = local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self, ), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      local_derivative = 1.0 if out.data == 0 else out.data\n",
        "      self.grad += local_derivative * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "    topo = list(reversed(topo))\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in topo:\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLrOAG2Ne6c1"
      },
      "source": [
        "`Value` type is changed and now we need to reevaluate `ypred`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiPmRMnzexCP",
        "outputId": "8b025797-c439-4f10-a061-99d45a191747"
      },
      "outputs": [],
      "source": [
        "ypred = [n(x) for x in xs]\n",
        "loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rxnEu3Fe0v-"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6aJa8jPfhZx"
      },
      "source": [
        "What happened when we called the `backward` method? Each weight of each neuron now has its `grad` computed. It tells us about the impact of that weight on the loss function and the direction it should be tweaked in to reduce the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPBhc9LfQct",
        "outputId": "cb2f51de-7761-414e-b568-1b56ba23b426"
      },
      "outputs": [],
      "source": [
        "n.layers[0].neurons[0].w[0].grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7L3zX5t5fVlw",
        "outputId": "74d21688-1573-49fe-a13d-55def74b7ca0"
      },
      "outputs": [],
      "source": [
        "draw_dot(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEF4fdgBmrvC"
      },
      "source": [
        "> _**Intuition**:_ Training the network is kinda like tweaking the radio knobs until you find the right station. You move weights up or down by using the gradient values until the loss approaches zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEerHC_4vhbv"
      },
      "source": [
        "## Collecting all of the parameters of the neural net [01:57:56](https://www.youtube.com/watch?v=VMj-3S1tku0&t=7076s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M12UZ4IDxODs"
      },
      "source": [
        "Time for some convenience code. We'll collect all the weights and biases for every neuron, so that we can nudge them in the right direction after every backward pass simultaneously. These are often called _network parameters_.\n",
        "\n",
        "Following the `PyTorch` convention, we'll add a `parameters` method to our `Neuron`, `Layer` and `MLP` classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2IQUE2bgd1o"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "  def __init__(self, nin):\n",
        "    \"\"\"\n",
        "      nin: number of inputs\n",
        "    \"\"\"\n",
        "    self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1, 1))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # w * x + b\n",
        "    activation = sum([wi*xi for wi, xi in zip(self.w, x)], self.b)\n",
        "    out = activation.tanh()\n",
        "\n",
        "    return out\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.w + [self.b]\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    # as a matter of convenience, unwrap the value\n",
        "    # out of array if it's just a single value.\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
        "\n",
        "class MLP:\n",
        "\n",
        "  def __init__(self, nin, nouts):\n",
        "    sizes = [nin] + nouts\n",
        "    self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noF-0Pokyry7",
        "outputId": "0d659748-3353-4003-ac86-6a0b5d886648"
      },
      "outputs": [],
      "source": [
        "x = [2.0, 3.0, -1.0]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "n(x)\n",
        "\n",
        "print(f\"# Parameters: {len(n.parameters())}\")\n",
        "n.parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lrDF3XZ6l8P"
      },
      "source": [
        "## Doing gradient descent optimization manually, training the network [02:01:12](https://www.youtube.com/watch?v=VMj-3S1tku0&t=7272s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAwwH0p_63i4"
      },
      "source": [
        "_Gradient descent_ iteratively applies the _forward pass_, the _backward pass_ and _weight update step_ to minimize the _loss function_.\n",
        "\n",
        "In _gradient descent_ we think about the gradient as of a vector pointing in the direction of increased loss. Since we want to minimize loss, at every gradient descent step we want to move in the opposite direction - _away from loss_ :) In numerical terms this means increasing a weight if its gradient is negative and decreasing it otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_BgeMMB822W"
      },
      "source": [
        "The training loop basically consists of three steps:\n",
        "\n",
        "1. Forward pass: compute all the neuron values, given the input data;\n",
        "\n",
        "2. Backward pass: Compute the gradients for each parameter;\n",
        "\n",
        "3. Adjust weights and biases: Use gradient to nudge parameters in the right direction, away from maximum loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28A1JP9AtySm"
      },
      "source": [
        "Go ahead and repeat these steps a few times to see how the loss is gradually decreased:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG4D8-jW8qDe",
        "outputId": "1ff25edc-8b8a-4042-82fb-efb98ab38d43"
      },
      "outputs": [],
      "source": [
        "# Forward pass\n",
        "ypred = [n(x) for x in xs]\n",
        "loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R80NSEJD8txe"
      },
      "outputs": [],
      "source": [
        "# Backward pass\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFPYC8Eb4sY4"
      },
      "outputs": [],
      "source": [
        "# Adjust weights and biases\n",
        "for p in n.parameters():\n",
        "  p.data += -1 * 0.05 * p.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajCPzQQ9mkS"
      },
      "source": [
        "Let's wrap these steps into a training loop for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7aHZBh98xLD",
        "outputId": "ea7e7d38-0b4d-4675-f1f7-a6cb15ce686b"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 20):\n",
        "  ypred = [n(x) for x in xs]\n",
        "  loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "  loss.backward()\n",
        "  for p in n.parameters():\n",
        "    p.data += -1 * 0.05 * p.grad\n",
        "  print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOSodo8w_xr-"
      },
      "source": [
        "And we can see that the predicted values get pretty close to ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysl_IxQl_Uoz",
        "outputId": "e62b0037-2c5b-4346-deb8-423e5b338438"
      },
      "outputs": [],
      "source": [
        "[n(x) for x in xs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC11jwPhzFLf"
      },
      "source": [
        "> **_Note about the step size_**: Can't you just pick a big step size to make the neural net learn faster? Why do we use just `0.05`?\n",
        ">\n",
        "> Remember that at each step we are operating at a local point of the loss function curve. We don't really know its shape. It may be complicated. Picking an overly large step size would likely lead the network to overshoot. It's like _jumping to conclusions_ - updating your beliefs _too fast_, based on small and inconclusive hints.\n",
        ">\n",
        "> Should we make the learning rate much smaller? Not necessarily. Set it too small and your neural net will take too long to train. For large LLMs that means considerable unnecessary costs.\n",
        ">\n",
        "> So aim to set your learning rate to a value that's _about right_. In practice, it's a bit of an art done through trial-and-error. There are rules of thumb that Andrej touches in later lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4NvcOb2_55l"
      },
      "source": [
        "## _Gotcha!_ Fixing yet another common bug [02:10:23](https://youtu.be/VMj-3S1tku0?t=7823)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47nyJvZJ2lFY"
      },
      "source": [
        "Our training loop code has a bug that's both subtle and common. It's a case of _you forgot to `zero_grad()`_ - essentially, we're not resetting the gradient on each training loop iteration and it keeps accumulating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "4cV7bPMH_sxj",
        "outputId": "6ab100b8-eadc-47a3-de5d-d5f7e86bb463"
      },
      "outputs": [],
      "source": [
        "Image(url='https://miro.medium.com/v2/resize:fit:800/0*NUjEE5SqiyVCRvLR.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgVYeJJyByn6"
      },
      "source": [
        "The interesting thing is that our network still converged. Why is that?\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "First, our network is very simple. Second, we got lucky. If we were training a network for a more difficult problem, most likely it wouldn't converge. Let's fix that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWbnrDWZDZpW"
      },
      "outputs": [],
      "source": [
        "n = MLP(3, [4, 4, 1]) # reset the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQTYTQFsBYMP",
        "outputId": "1fccdced-3f45-4203-97d6-a9184ec367c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(0, 20):\n",
        "  # Reset gradients to zero.\n",
        "  # We don't want to keep the gradients from previous iteration: we're now at a new point on the loss curve.\n",
        "  for p in n.parameters():\n",
        "    p.grad = 0.0\n",
        "\n",
        "  ypred = [n(x) for x in xs]\n",
        "  loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "\n",
        "\n",
        "  loss.backward()\n",
        "  for p in n.parameters():\n",
        "    p.data += -1 * 0.05 * p.grad\n",
        "  print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW7JD1CXDikY"
      },
      "source": [
        "That's it. Our `micrograd`-based neural network is complete!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wajZOOLGpNn"
      },
      "source": [
        "# Summary of what we learned, how to go towards modern neural nets [02:14:03](https://www.youtube.com/watch?v=VMj-3S1tku0&t=8043s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBv98NOFeW83"
      },
      "source": [
        "The key takeaways:\n",
        "\n",
        "1. _Neural nets_ are mathematical expressions. They take _vectors of input data_ `x`, feed them forward through a _computational graph_ and into a non-linear function like `tanh` to compute the _loss function_. The _loss function_ measures accuracy of network predictions. Its value will be low when your network behaves well.\n",
        "\n",
        "2. A _neuron_ is a node of the _computational graph_. It has its _weights_ and a _bias_ that determine how inputs are transformed.\n",
        "\n",
        "3. A neural network is trained by iteratively computing _gradients_ of each node during the _backward pass_ to figure out how that impacts _loss_ of the entire network. These gradients are used to tweak the individual _weights_ and _biases_ in order to minimize the _loss function_. This optimization algorithm is called _gradient descent_.\n",
        "\n",
        "We built a very small network with just `41` parameters. But you can build much larger networks with _billions_ of parameters. As the size of the network and training data goes up, neural networks start to show fascinating emergent properties and capacity to solve complex problems.\n",
        "\n",
        "Now you have an intuitive understanding how neural networks work under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJHjlsqrgT6J"
      },
      "source": [
        "# Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_29lxabgbh-"
      },
      "source": [
        "1. Go through the [micrograd library code](https://github.com/karpathy/micrograd) on Github [02:16:46](https://www.youtube.com/watch?v=VMj-3S1tku0&t=8206s). Most of the code should now be easy to recognize. It features a slightly more complicated demo example in [demo.ipynb](https://github.com/karpathy/micrograd/blob/master/demo.ipynb)\n",
        "\n",
        "2. Complete these excercises on your own to test your knowledge: [micrograd_exercises.ipynb](https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing)\n",
        "\n",
        "3. Have a look at how operations like `tanh` are [implemented in PyTorch](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp) and how new functions can be added like Lego blocks. [02:21:10](https://www.youtube.com/watch?v=VMj-3S1tku0&t=8470s)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
